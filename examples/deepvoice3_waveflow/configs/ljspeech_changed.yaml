meta_data:
  min_text_length: 20

transform:
  # text
  replace_pronunciation_prob: 0.5

  # spectrogram
  sample_rate: 22050
  max_norm: 0.999
  preemphasis: 0.97
  n_fft: 1024
  win_length: 1024
  hop_length: 256

  # mel
  # fmin: 125
  # fmax: 7600
  fmin: 0.0
  fmax: 8000.0
  n_mels: 80

  # db scale
  min_level_db: -100
  ref_level_db: 20
  clip_norm: true


loss:
  masked_loss_weight: 0.5
  priority_freq: 3000
  priority_freq_weight: 0.0
  binary_divergence_weight: 0.1
  guided_attention_sigma: 0.2

synthesis:
  max_steps: 512
  power: 1.4
  n_iter: 32

model:
  # speaker_embedding
  n_speakers: 1
  speaker_embed_dim: 16
  speaker_embedding_weight_std: 0.01
  
  max_positions: 512
  dropout: 0.050000000000000044
  # encoder
  text_embed_dim: 256
  embedding_weight_std: 0.1
  freeze_embedding: false
  padding_idx: 0
  encoder_channels: 512

  # decoder
  query_position_rate: 1.0
  key_position_rate: 1.29
  trainable_positional_encodings: false
  kernel_size: 3
  decoder_channels: 256
  downsample_factor: 4
  outputs_per_step: 1
  
  # attention
  key_projection: true
  value_projection: true
  force_monotonic_attention: true
  window_backward: -1
  window_ahead: 3
  use_memory_mask: true

  # converter
  use_decoder_state_for_postnet_input: true
  converter_channels: 256

optimizer:
  beta1: 0.5
  beta2: 0.9
  epsilon: 1e-6

lr_scheduler:
  warmup_steps: 4000
  peak_learning_rate: 5e-4
  
train:
  batch_size: 16
  max_iteration: 2000000
  
  snap_interval: 1000
  eval_interval: 10000
  save_interval: 10000

waveflow:
  valid_size: 16
  segment_length: 16000
  #sample_rate: 8000
  sample_rate: 22050
  fft_window_shift: 256
  fft_window_size: 1024
  fft_size: 1024
  mel_bands: 80
  mel_fmin: 0.0
  mel_fmax: 8000.0

  seed: 1234
  learning_rate: 0.0002
  batch_size: 8
  test_every: 2000
  save_every: 10000
  max_iterations: 3000000

  sigma: 1.0
  n_flows: 8
  n_group: 16
  n_layers: 8
  n_channels: 64
  kernel_h: 3
  kernel_w: 3

